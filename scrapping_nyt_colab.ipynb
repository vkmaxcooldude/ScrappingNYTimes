{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scrapping_NYT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "0bfZR1VRfnsH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "b03f049f-9231-4344-df3d-7927a94cb7f2"
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Author: Vishal K (vkmaxcooldude)\n",
        "Python 3\n",
        "Using the BeautifulSoup and requests Python packages to print out a list of all the article titles, link and Summary\n",
        "on the New York Times homepage.\n",
        "Install BeautifulSoup, requests and lxml parser using pip cmd installer\n",
        "'''\n",
        "!pip install beautifulsoup4\n",
        "!pip install requests\n",
        "!pip install lxml\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "while True:\n",
        "    r = requests.get(\"https://www.nytimes.com/index.html\")\n",
        "    if r.status_code == requests.codes.ok:\n",
        "        break\n",
        "r_html = r.text\n",
        "soup = BeautifulSoup(r_html, 'lxml')\n",
        "title = soup.find_all('article')\n",
        "for x in title:\n",
        "    try:\n",
        "        print()\n",
        "        link = x.a['href']\n",
        "        print(\"HEADLINE: \" + x.a.h2.text)\n",
        "        print(\"   Link: https://www.nytimes.com\" + str(link))\n",
        "        try:\n",
        "            ordered_list = x.find('ul')\n",
        "            ctr = 0\n",
        "            for y in ordered_list:\n",
        "                ctr += 1\n",
        "                print(\"   \" + str(ctr) + \". \" + y.text)\n",
        "        except:\n",
        "            print(\"   \" + x.a.p.text)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "    print()\n",
        "for x in title:\n",
        "    try:\n",
        "        print()\n",
        "        link = x.a['href']\n",
        "        print(\"HEADLINE: \" + x.a.h2.span.text)\n",
        "        print(\"   Link: https://www.nytimes.com\" + str(link))\n",
        "        try:\n",
        "            ordered_list = x.find('ul')\n",
        "            ctr = 0\n",
        "            for y in ordered_list:\n",
        "                ctr += 1\n",
        "                print(\"   \" + str(ctr) + \". \" + y.text)\n",
        "        except:\n",
        "            print(\"   \" + x.a.p.text)\n",
        "    except:\n",
        "        continue\n",
        "    print()\n",
        "    "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FeatureNotFound",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0168c09fce87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mr_html\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_html\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m#print(soup.prettify())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'article'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m                     \u001b[0;34m\"Couldn't find a tree builder with the features you \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0;34m\"requested: %s. Do you need to install a parser library?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m                     % \",\".join(features))\n\u001b[0m\u001b[1;32m    199\u001b[0m             \u001b[0mbuilder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             if not (original_features == builder.NAME or\n",
            "\u001b[0;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
          ]
        }
      ]
    }
  ]
}